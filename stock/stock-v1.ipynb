{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n\n!if [ ! -e \"stocks\" ]; then \\\n  gdown --id '1g5c7kmbf2Xqp8O6ghPxmx0RaOjWc6SBu' --output \"stocks.zip\" ;\\\n  unzip -q \"stocks.zip\" ;\\\nfi","metadata":{"id":"pFUfv-54i9Sa","outputId":"a79590b0-453a-4250-f7ad-059f6fb29d6f","execution":{"iopub.status.busy":"2022-11-08T13:25:59.567828Z","iopub.execute_input":"2022-11-08T13:25:59.568988Z","iopub.status.idle":"2022-11-08T13:26:12.941435Z","shell.execute_reply.started":"2022-11-08T13:25:59.568942Z","shell.execute_reply":"2022-11-08T13:26:12.939502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:26:12.944896Z","iopub.execute_input":"2022-11-08T13:26:12.945510Z","iopub.status.idle":"2022-11-08T13:26:13.987587Z","shell.execute_reply.started":"2022-11-08T13:26:12.945450Z","shell.execute_reply":"2022-11-08T13:26:13.986024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!if [ ! -e \"stock_num.txt\" ]; then \\\n  gdown --id '18jgYbCxvOzHWVwwFK3G-o1yU34GUnuj1' --output \"stock_num.txt\" ;\\\nfi","metadata":{"id":"RdN-JZMJ_cGG","outputId":"4bafd2c3-8756-444b-c6a9-b6b13e04ddfa","execution":{"iopub.status.busy":"2022-11-08T13:26:13.989800Z","iopub.execute_input":"2022-11-08T13:26:13.990797Z","iopub.status.idle":"2022-11-08T13:26:15.033316Z","shell.execute_reply.started":"2022-11-08T13:26:13.990732Z","shell.execute_reply":"2022-11-08T13:26:15.031706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\n\n# Reading/Writing Data\nimport pandas as pd\nimport os\nimport csv\n\n# For Progress Bar\nfrom tqdm import tqdm\n\n# Pytorch\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset, SubsetRandomSampler\n\n# KFold\n# KFOLD Reference: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\nfrom sklearn.model_selection import KFold\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For plotting learning curve\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"id":"PgBXnwiNjgvV","execution":{"iopub.status.busy":"2022-11-08T13:26:15.037762Z","iopub.execute_input":"2022-11-08T13:26:15.038501Z","iopub.status.idle":"2022-11-08T13:26:15.048343Z","shell.execute_reply.started":"2022-11-08T13:26:15.038434Z","shell.execute_reply":"2022-11-08T13:26:15.046849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_num = list()\nwith open('stock_num.txt') as f:\n    lines = f.readlines()\n    for line in lines:\n        stock_num.append(line.strip())","metadata":{"id":"dniOWWe1s5h9","execution":{"iopub.status.busy":"2022-11-08T13:26:15.050509Z","iopub.execute_input":"2022-11-08T13:26:15.051055Z","iopub.status.idle":"2022-11-08T13:26:15.066848Z","shell.execute_reply.started":"2022-11-08T13:26:15.051004Z","shell.execute_reply":"2022-11-08T13:26:15.065515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_num_mod = []\nfor i in range(len(stock_num)):\n    path = 'stocks/' + stock_num[i] + '.csv'  \n    try:\n        stock = pd.read_csv(path)\n        if len(stock) >= 500:\n            stock_num_mod.append(stock_num[i])\n    except:\n        print(path, \" is empty and has been skipped.\")    \n\nprint(len(stock_num_mod))","metadata":{"id":"FGqKZ3Drjz2z","outputId":"446f64b4-b1d0-4cc2-9866-7fdfd5f19a03","execution":{"iopub.status.busy":"2022-11-08T13:26:16.087837Z","iopub.execute_input":"2022-11-08T13:26:16.089220Z","iopub.status.idle":"2022-11-08T13:26:19.211554Z","shell.execute_reply.started":"2022-11-08T13:26:16.089158Z","shell.execute_reply":"2022-11-08T13:26:19.210155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def same_seed(seed): \n    '''Fixes random number generator seeds for reproducibility.'''\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef train_valid_split(data_set, valid_ratio, seed):\n    '''Split provided training data into training set and validation set'''\n    valid_set_size = int(valid_ratio * len(data_set)) \n    train_set_size = len(data_set) - valid_set_size\n    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n    return np.array(train_set), np.array(valid_set)\n\ndef predict(test_loader, model, device):\n    model.eval() # Set your model to evaluation mode.\n    preds = []\n    for x in tqdm(test_loader):\n        x = x.to(device)                        \n        with torch.no_grad():                   \n            pred = model(x)                     \n            preds.append(pred.detach().cpu())   \n    preds = torch.cat(preds, dim=0).numpy()  \n    return preds","metadata":{"id":"7PveUY_FmAlM","execution":{"iopub.status.busy":"2022-11-08T13:26:20.345695Z","iopub.execute_input":"2022-11-08T13:26:20.346224Z","iopub.status.idle":"2022-11-08T13:26:20.357651Z","shell.execute_reply.started":"2022-11-08T13:26:20.346180Z","shell.execute_reply":"2022-11-08T13:26:20.356271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class My_Model(nn.Module):\n    def __init__(self, input_dim):\n        super(My_Model, self).__init__()        \n        self.layers = nn.Sequential(\n            nn.Linear(input_dim, 8),            \n            nn.ReLU(),            \n            #nn.Linear(16, 8),\n            #nn.ReLU(),\n            nn.Linear(8, 4),\n            nn.ReLU(),\n            #nn.Linear(4, 2),\n            #nn.ReLU(),\n            nn.Linear(4, 1)\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        #x = x.squeeze(1) # (B, 1) -> (B)\n        #for i in range(len(x)):\n        #    if x[i] > 0:\n            #if x[i] > 0.006:\n        #        x[i] = 1\n        #    else: x[i] = 0\n        return x","metadata":{"id":"24JpIPBipoOt","execution":{"iopub.status.busy":"2022-11-08T13:26:24.348461Z","iopub.execute_input":"2022-11-08T13:26:24.348995Z","iopub.status.idle":"2022-11-08T13:26:24.360361Z","shell.execute_reply.started":"2022-11-08T13:26:24.348952Z","shell.execute_reply":"2022-11-08T13:26:24.358471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Stock:\n    def __init__(self, *stock_numbers):\n        self.stock_numbers = stock_numbers    \n    def scrape(self):\n\n        for stock_number in self.stock_numbers:\n            path = 'stocks/' + stock_number + '.csv'           \n            data = pd.read_csv(path)\n            \n            incl = list()\n            prel = list()\n            max9 = list()\n            min9 = list()            \n            rsvl = list()\n            kl = list()\n            dl = list()\n            jl = list()\n            avgl = list()\n            biasl = list()\n            for i in range(8):\n                max9.append(\"0\")\n                min9.append(\"0\")                \n                rsvl.append(\"0\")\n                \n            for i in range(7):\n                kl.append(\"0\")\n                dl.append(\"0\")\n                jl.append(\"0\")\n            \n            incl.append(\"0\")\n            avgl.append(\"0\")\n            biasl.append(\"0\")\n                \n            kl.append(\"50\")\n            dl.append(\"50\")\n            jl.append(\"50\")\n                \n            for i in range(len(data)-8):\n                dmax = data[['max']].iloc[i:i+9].max().to_string(index=False)\n                max9.append(dmax)\n                    \n                dmin = data[['min']].iloc[i:i+9].min().to_string(index=False)\n                min9.append(dmin)\n            \n            for i in range(8, len(data)):\n                close = data[['close']].iloc[i].to_string(index=False)\n                if float(max9[i])-float(min9[i]) != 0.0:\n                    rsv = ((float(close)-float(min9[i]))/(float(max9[i])-float(min9[i])))                  \n                else: rsv = float(kl[i-1])/100\n                k = ((2/3)*float(kl[i-1]))+((100/3)*rsv)\n                d = ((2/3)*float(dl[i-1]))+((1/3)*k)\n                j = ((3*d)-(2*k))\n                rsvl.append(rsv)\n                kl.append(k)\n                dl.append(d)\n                jl.append(j)\n            \n            for i in range(len(data)-1):\n                close1 = data[['close']].iloc[i].to_string(index=False)\n                close2 = data[['close']].iloc[i+1].to_string(index=False)\n                if float(close1) != 0.0:\n                    inc = (100*((float(close2)-float(close1))/float(close1)))\n                    incl.append(inc)\n                    prel.append(inc)\n                else:\n                    incl.append(\"0\")\n                    prel.append(\"0\")\n                avg = data[['close']].iloc[i:i+2].mean().to_string(index=False)\n\n                \n                avgl.append(avg)\n            \n            for i in range(1, len(data)):\n                close = data[['close']].iloc[i].to_string(index=False)\n                if float(avgl[i]) != 0.0:\n                    bias = (100*((float(close)-float(avgl[i]))/float(avgl[i])))\n                    biasl.append(bias)\n                else: biasl.append(\"0\")\n            \n            prel.append(\"0\")\n            \n            k1l = kl.copy()\n            d1l = dl.copy()\n            j1l = jl.copy()\n            k2l = kl.copy()\n            d2l = dl.copy()\n            j2l = jl.copy()         \n            \n            k1l.insert(0, \"0\")\n            d1l.insert(0, \"0\")\n            j1l.insert(0, \"0\")\n            k1l.pop()\n            d1l.pop()\n            j1l.pop()\n            \n            for i in range(2):\n                k2l.insert(0, \"0\")\n                d2l.insert(0, \"0\")\n                j2l.insert(0, \"0\")\n                k2l.pop()\n                d2l.pop()\n                j2l.pop()\n                        \n            data['inc'] = incl            \n            data['max9'] = max9\n            data['min9'] = min9\n            data['rsv'] = rsvl\n            data['k'] = kl\n            data['d'] = dl\n            data['j'] = jl\n            data['avg'] = avgl\n            data['bias'] = biasl\n            data['k1'] = k1l\n            data['d1'] = d1l\n            data['j1'] = j1l\n            data['k2'] = k2l\n            data['d2'] = d2l\n            data['j2'] = j2l\n            \n            if len(data)>=160:\n                ema12l = list()\n                ema26l = list()\n                difl = list()\n                macdl = list()\n                dif_macdl = list()\n            \n                for i in range(11):\n                    ema12l.append(\"0\")\n                \n                for i in range(25):\n                    ema26l.append(\"0\")\n                    difl.append(\"0\")                    \n                \n                di12 = (float(data[['max']].iloc[:12].mean().to_string(index=False))+float(data[['min']].iloc[:12].mean().to_string(index=False))+float(data[['close']].iloc[:12].mean().to_string(index=False))*2)/4\n                di26 = (float(data[['max']].iloc[:26].mean().to_string(index=False))+float(data[['min']].iloc[:26].mean().to_string(index=False))+float(data[['close']].iloc[:26].mean().to_string(index=False))*2)/4\n\n                \n                ema12l.append(di12)\n                ema26l.append(di26)\n                \n                for i in range(12, len(data)):\n                    close = data[['close']].iloc[i].to_string(index=False)\n                    max1 = data[['max']].iloc[i].to_string(index=False)\n                    min1 = data[['min']].iloc[i].to_string(index=False)\n                    di = (float(close)*2+float(max1)+float(min1))/4\n                    ema12today = (float(ema12l[i-1])*11 + di*2)/13\n                    ema12l.append(ema12today)\n                \n                for i in range(26, len(data)):\n                    close = data[['close']].iloc[i].to_string(index=False)\n                    max1 = data[['max']].iloc[i].to_string(index=False)\n                    min1 = data[['min']].iloc[i].to_string(index=False)\n                    di = (float(close)*2+float(max1)+float(min1))/4\n                    ema26today = (float(ema26l[i-1])*25 + di*2)/27\n                    ema26l.append(ema26today)\n                \n                for i in range(25, len(data)):\n                    difl.append(float(ema12l[i])-float(ema26l[i]))\n                    \n                for i in range(33):\n                    macdl.append(\"0\")\n                    \n                macdl.append(sum(difl[25:34])/9)\n                \n                for i in range(34, len(data)):\n                    macdtoday = (float(macdl[i-1])*8 + float(difl[i])*2)/10\n                    macdl.append(macdtoday)\n                    \n                for i in range(len(data)):\n                    dif_macdl.append(float(difl[i])-float(macdl[i]))\n                 \n                    \n                data['ema12'] = ema12l\n                data['ema26'] = ema26l\n                data['dif'] = difl\n                data['macd'] = macdl\n                data['dif-macd'] = dif_macdl\n                \n                data['pre'] = prel\n            \n        return data.iloc[40:]","metadata":{"id":"sm7gWTkep3kd","execution":{"iopub.status.busy":"2022-11-08T13:26:26.788600Z","iopub.execute_input":"2022-11-08T13:26:26.789848Z","iopub.status.idle":"2022-11-08T13:26:26.837457Z","shell.execute_reply.started":"2022-11-08T13:26:26.789774Z","shell.execute_reply":"2022-11-08T13:26:26.835533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock = Stock('2330')\ndata = stock.scrape()","metadata":{"id":"9S9c9OdSrRhn","execution":{"iopub.status.busy":"2022-11-08T13:26:33.300458Z","iopub.execute_input":"2022-11-08T13:26:33.301487Z","iopub.status.idle":"2022-11-08T13:26:46.216507Z","shell.execute_reply.started":"2022-11-08T13:26:33.301437Z","shell.execute_reply":"2022-11-08T13:26:46.215202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kdif = list()\nkdif2 = list()\nfor i in range(len(data)):\n    k = data['k'].iloc[i]\n    k1 = data['k1'].iloc[i]\n    k2 = data['k2'].iloc[i]\n    kdif.append(float(k) - float(k1))\n    kdif2.append(float(k1) - float(k2))\ndata['kdif'] = kdif\ndata['kdif2'] = kdif2","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:26:46.218747Z","iopub.execute_input":"2022-11-08T13:26:46.219185Z","iopub.status.idle":"2022-11-08T13:26:46.263511Z","shell.execute_reply.started":"2022-11-08T13:26:46.219149Z","shell.execute_reply":"2022-11-08T13:26:46.261895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"C7yJ-RgEu0CO","outputId":"94b7be61-3959-4b49-8f31-15a847657c61","execution":{"iopub.status.busy":"2022-11-08T13:26:46.265644Z","iopub.execute_input":"2022-11-08T13:26:46.266371Z","iopub.status.idle":"2022-11-08T13:26:46.300208Z","shell.execute_reply.started":"2022-11-08T13:26:46.266298Z","shell.execute_reply":"2022-11-08T13:26:46.298828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names = list(data.columns.values)\ncolumn_names","metadata":{"id":"n5aXtM6OwhPO","outputId":"88289d20-9443-4663-82cd-c230e9b8d65e","execution":{"iopub.status.busy":"2022-11-08T13:26:46.303269Z","iopub.execute_input":"2022-11-08T13:26:46.303738Z","iopub.status.idle":"2022-11-08T13:26:46.317189Z","shell.execute_reply.started":"2022-11-08T13:26:46.303677Z","shell.execute_reply":"2022-11-08T13:26:46.315986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"id":"AMRTj2zEttQ9","outputId":"8bc51ced-1cd5-452c-ab18-d0f5972df69f","execution":{"iopub.status.busy":"2022-11-08T13:26:46.318806Z","iopub.execute_input":"2022-11-08T13:26:46.319178Z","iopub.status.idle":"2022-11-08T13:26:46.330985Z","shell.execute_reply.started":"2022-11-08T13:26:46.319146Z","shell.execute_reply":"2022-11-08T13:26:46.329840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"id":"YXhxmqdyx3Te","outputId":"67c0f632-924c-4ada-a9d8-2c46b8531c68","execution":{"iopub.status.busy":"2022-11-08T13:26:50.731454Z","iopub.execute_input":"2022-11-08T13:26:50.731930Z","iopub.status.idle":"2022-11-08T13:26:50.743430Z","shell.execute_reply.started":"2022-11-08T13:26:50.731894Z","shell.execute_reply":"2022-11-08T13:26:50.741852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop('stock_id', inplace=True, axis=1)","metadata":{"id":"kmiL5NUiwYJn","execution":{"iopub.status.busy":"2022-11-08T13:26:53.286348Z","iopub.execute_input":"2022-11-08T13:26:53.286762Z","iopub.status.idle":"2022-11-08T13:26:53.294422Z","shell.execute_reply.started":"2022-11-08T13:26:53.286729Z","shell.execute_reply":"2022-11-08T13:26:53.293449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.astype({'inc':'float','pre':'float','max9':'float','min9':'float','rsv':'float','k':'float','d':'float','j':'float','avg':'float','bias':'float','ema12':'float','ema26':'float','dif':'float','macd':'float','k1':'float','d1':'float','j1':'float','k2':'float','d2':'float','j2':'float'})","metadata":{"id":"maYWjeeCya-M","execution":{"iopub.status.busy":"2022-11-08T13:26:54.758274Z","iopub.execute_input":"2022-11-08T13:26:54.758663Z","iopub.status.idle":"2022-11-08T13:26:54.776796Z","shell.execute_reply.started":"2022-11-08T13:26:54.758632Z","shell.execute_reply":"2022-11-08T13:26:54.774750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()","metadata":{"id":"CQMR76WfubpN","outputId":"f8078ecd-d69a-4c7f-8df5-5e0f3a7c3f69","execution":{"iopub.status.busy":"2022-11-08T13:26:56.324553Z","iopub.execute_input":"2022-11-08T13:26:56.325037Z","iopub.status.idle":"2022-11-08T13:26:56.376258Z","shell.execute_reply.started":"2022-11-08T13:26:56.324998Z","shell.execute_reply":"2022-11-08T13:26:56.374576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 19))\np = sns.heatmap(data.corr(), annot=True)","metadata":{"id":"5pDAF4Y8ranG","outputId":"955eee6c-bf9e-43ab-a182-3596278054d3","execution":{"iopub.status.busy":"2022-11-08T13:26:59.951830Z","iopub.execute_input":"2022-11-08T13:26:59.952259Z","iopub.status.idle":"2022-11-08T13:27:05.206104Z","shell.execute_reply.started":"2022-11-08T13:26:59.952224Z","shell.execute_reply":"2022-11-08T13:27:05.204595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing\n#x = np.empty([10 * 200, 5 * 5], dtype = float)\n#y = np.empty([10 * 200, 1], dtype = float)\n#xv = np.empty([10 * 50, 5 * 5], dtype = float)\n#yv = np.empty([10 * 50, 1], dtype = float)\n#test = np.empty([10, 5 * 5], dtype = float)\n\n#train\nx = np.empty([len(stock_num_mod) * 200, 5 * 5], dtype = float)\ny = np.empty([len(stock_num_mod) * 200, 1], dtype = float)\nxv = np.empty([len(stock_num_mod) * 50, 5 * 5], dtype = float)\nyv = np.empty([len(stock_num_mod) * 50, 1], dtype = float)\ntest = np.empty([len(stock_num_mod), 5 * 5], dtype = float)\n\n#testing\n#for i in range(10):\n#train\nfor i in range(len(stock_num_mod)):\n    stock = Stock(stock_num_mod[i])\n    #print(i)\n    data = stock.scrape()\n    for n in range(len(data)-51, len(data)-251, -1):        \n        if data['close'].iloc[n-1] != 0.0:\n            y[i*200 +(len(data)-n-51)] = (data['close'].iloc[n]-data['close'].iloc[n-1])/data['close'].iloc[n-1]\n        for m in range(5):            \n            x[i*200 +(len(data)-n-51), m] = data['k'].iloc[n-m-1]\n            x[i*200 +(len(data)-n-51), m+5] = data['d'].iloc[n-m-1]\n            x[i*200 +(len(data)-n-51), m+10] = data['j'].iloc[n-m-1]\n            x[i*200 +(len(data)-n-51), m+15] = data['bias'].iloc[n-m-1]\n            x[i*200 +(len(data)-n-51), m+20] = float(data['dif'].iloc[n-m-1]) - float(data['macd'].iloc[n-m-1])\n                \n    for n in range(len(data)-1, len(data)-51, -1):        \n        if data['close'].iloc[n-1] != 0.0:\n            yv[i*50 +(len(data)-n-1)] = (data['close'].iloc[n]-data['close'].iloc[n-1])/data['close'].iloc[n-1]\n        for m in range(5):            \n            xv[i*50 +(len(data)-n-1), m] = data['k'].iloc[n-m-1]\n            xv[i*50 +(len(data)-n-1), m+5] = data['d'].iloc[n-m-1]\n            xv[i*50 +(len(data)-n-1), m+10] = data['j'].iloc[n-m-1]\n            xv[i*50 +(len(data)-n-1), m+15] = data['bias'].iloc[n-m-1]\n            xv[i*50 +(len(data)-n-1), m+20] = float(data['dif'].iloc[n-m-1]) - float(data['macd'].iloc[n-m-1])\n            \n    for m in range(5):        \n        test[i, m] = data['k'].iloc[len(data)-m-1]\n        test[i, m+5] = data['d'].iloc[len(data)-m-1]\n        test[i, m+10] = data['j'].iloc[len(data)-m-1]\n        test[i, m+15] = data['bias'].iloc[len(data)-m-1]\n        test[i, m+20] = float(data['dif'].iloc[n-m-1]) - float(data['macd'].iloc[n-m-1])","metadata":{"id":"y6VbQGBvi4sJ","execution":{"iopub.status.busy":"2022-11-08T13:27:05.208692Z","iopub.execute_input":"2022-11-08T13:27:05.209226Z","iopub.status.idle":"2022-11-08T13:29:12.975048Z","shell.execute_reply.started":"2022-11-08T13:27:05.209179Z","shell.execute_reply":"2022-11-08T13:29:12.973644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = x\nY_train = y\nX_dev = xv\nY_dev = yv\nX_test = test\nfor i in range(len(Y_train)):\n    if Y_train[i] > 0:\n    #if Y_train[i] > 0.006:\n        Y_train[i] = 1\n    else: Y_train[i] = 0 \nfor i in range(len(Y_dev)):\n    if Y_dev[i] > 0:\n    #if Y_dev[i] > 0.006:\n        Y_dev[i] = 1\n    else: Y_dev[i] = 0 \ntrain_size = X_train.shape[0]\ndev_size = X_dev.shape[0]\ntest_size = X_test.shape[0]\ndata_dim = X_train.shape[1]\nprint('Size of training set: {}'.format(train_size))\nprint('Size of development set: {}'.format(dev_size))\nprint('Size of testing set: {}'.format(test_size))\nprint('Dimension of data: {}'.format(data_dim))","metadata":{"id":"mPTK6bEhi4sN","execution":{"iopub.status.busy":"2022-11-08T13:29:12.977275Z","iopub.execute_input":"2022-11-08T13:29:12.977715Z","iopub.status.idle":"2022-11-08T13:29:12.994704Z","shell.execute_reply.started":"2022-11-08T13:29:12.977668Z","shell.execute_reply":"2022-11-08T13:29:12.992976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y1 = 100*y\n#yv1 = 100*yv","metadata":{"id":"Cb0Pp6Qji4sM","execution":{"iopub.status.busy":"2022-10-31T07:13:37.557348Z","iopub.execute_input":"2022-10-31T07:13:37.557810Z","iopub.status.idle":"2022-10-31T07:13:37.565161Z","shell.execute_reply.started":"2022-10-31T07:13:37.557767Z","shell.execute_reply":"2022-10-31T07:13:37.563788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n    # This function normalizes specific columns of X.\n    # The mean and standard variance of training data will be reused when processing testing data.\n    #\n    # Arguments:\n    #     X: data to be processed\n    #     train: 'True' when processing training data, 'False' for testing data\n    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n    #         will be normalized.\n    #     X_mean: mean value of training data, used when train = 'False'\n    #     X_std: standard deviation of training data, used when train = 'False'\n    # Outputs:\n    #     X: normalized data\n    #     X_mean: computed mean value of training data\n    #     X_std: computed standard deviation of training data\n\n    #if specified_column == None:\n    #    specified_column = np.arange(X.shape[1])\n    if train:\n        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n\n    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n     \n    return X, X_mean, X_std\n\ndef _train_dev_split(X, Y, dev_ratio = 0.25):\n    # This function spilts data into training set and development set.\n    train_size = int(len(X) * (1 - dev_ratio))\n    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n\n# Normalize training and testing data\nspecified_column1 = np.arange(5)\nspecified_column2 = np.arange(5, 10)\nspecified_column3 = np.arange(10, 15)\nspecified_column4 = np.arange(15, 20)\nspecified_column5 = np.arange(20, 25)\nX_train, X_mean, X_std = _normalize(X_train, train = True, specified_column = specified_column1)\nX_test, _, _= _normalize(X_test, train = False, specified_column = specified_column1, X_mean = X_mean, X_std = X_std)\nX_train, X_mean, X_std = _normalize(X_train, train = True, specified_column = specified_column2)\nX_test, _, _= _normalize(X_test, train = False, specified_column = specified_column2, X_mean = X_mean, X_std = X_std)\nX_train, X_mean, X_std = _normalize(X_train, train = True, specified_column = specified_column3)\nX_test, _, _= _normalize(X_test, train = False, specified_column = specified_column3, X_mean = X_mean, X_std = X_std)\nX_train, X_mean, X_std = _normalize(X_train, train = True, specified_column = specified_column4)\nX_test, _, _= _normalize(X_test, train = False, specified_column = specified_column4, X_mean = X_mean, X_std = X_std)\nX_train, X_mean, X_std = _normalize(X_train, train = True, specified_column = specified_column5)\nX_test, _, _= _normalize(X_test, train = False, specified_column = specified_column5, X_mean = X_mean, X_std = X_std)\n\nX_dev, X_mean, X_std = _normalize(X_dev, train = True, specified_column = specified_column1)\nX_dev, X_mean, X_std = _normalize(X_dev, train = True, specified_column = specified_column2)\nX_dev, X_mean, X_std = _normalize(X_dev, train = True, specified_column = specified_column3)\nX_dev, X_mean, X_std = _normalize(X_dev, train = True, specified_column = specified_column4)\nX_dev, X_mean, X_std = _normalize(X_dev, train = True, specified_column = specified_column5)\n    \n# Split data into training set and development set\n#dev_ratio = 0.1\n#X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n\ntrain_size = X_train.shape[0]\ndev_size = X_dev.shape[0]\ntest_size = X_test.shape[0]\ndata_dim = X_train.shape[1]\nprint('Size of training set: {}'.format(train_size))\nprint('Size of development set: {}'.format(dev_size))\nprint('Size of testing set: {}'.format(test_size))\nprint('Dimension of data: {}'.format(data_dim))","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:29:12.997083Z","iopub.execute_input":"2022-11-08T13:29:12.997648Z","iopub.status.idle":"2022-11-08T13:29:13.029862Z","shell.execute_reply.started":"2022-11-08T13:29:12.997593Z","shell.execute_reply":"2022-11-08T13:29:13.028464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nconfig = {\n    'seed': 1314,      # Your seed number, you can pick your lucky number. :)\n    'select_all': False,   # Whether to use all features.\n    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n    'n_epochs': 2000,     # Number of epochs.            \n    'batch_size': 64, \n    'learning_rate': 5e-4,\n    'weight_decay': 5e-5,              \n    'early_stop': 200,    # If model has not improved for this many consecutive epochs, stop training.     \n    'k_folds': 5,\n    # 'save_path': './models/model.ckpt'  # Your model will be saved here.\n    'save_path': './models/'\n}","metadata":{"id":"A2jX4xmM9FP-","execution":{"iopub.status.busy":"2022-11-08T14:02:03.016088Z","iopub.execute_input":"2022-11-08T14:02:03.016573Z","iopub.status.idle":"2022-11-08T14:02:03.024830Z","shell.execute_reply.started":"2022-11-08T14:02:03.016535Z","shell.execute_reply":"2022-11-08T14:02:03.023143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StockDataset(Dataset):\n    '''\n    x: Features.\n    y: Targets, if none, do prediction.\n    '''\n    def __init__(self, x, y=None):\n        if y is None:\n            self.y = y\n        else:\n            self.y = torch.FloatTensor(y)\n        self.x = torch.FloatTensor(x)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x[idx]\n        else:\n            return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return len(self.x)","metadata":{"id":"LeakkuY59xXX","execution":{"iopub.status.busy":"2022-11-08T14:02:05.051098Z","iopub.execute_input":"2022-11-08T14:02:05.051608Z","iopub.status.idle":"2022-11-08T14:02:05.060922Z","shell.execute_reply.started":"2022-11-08T14:02:05.051569Z","shell.execute_reply":"2022-11-08T14:02:05.059263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set seed for reproducibility\nsame_seed(config['seed'])\n#train_data, valid_data = train_valid_split(train_data, config['valid_ratio'], config['seed'])\n\n# Print out the data size.\nprint(f\"\"\"train_data size: {X_train.shape} \nvalid_data size: {X_dev.shape} \ntest_data size: {X_test.shape}\"\"\")\n\n# Select features\n#x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['select_all'])\n\n# Print out the number of features.\nprint(f'number of features: {X_train.shape[1]}')\n\ntrain_dataset, valid_dataset, test_dataset = StockDataset(X_train, Y_train), \\\n                                            StockDataset(X_dev, Y_dev), \\\n                                            StockDataset(X_test)\n\n# For KFold\ndataset = ConcatDataset([train_dataset, valid_dataset])\n\n# Pytorch data loader loads pytorch dataset into batches.\n# Without KFold\ntrain_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)","metadata":{"id":"6KNfU18t9S8e","execution":{"iopub.status.busy":"2022-11-08T14:02:07.745260Z","iopub.execute_input":"2022-11-08T14:02:07.745748Z","iopub.status.idle":"2022-11-08T14:02:07.759609Z","shell.execute_reply.started":"2022-11-08T14:02:07.745710Z","shell.execute_reply":"2022-11-08T14:02:07.757927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainer(fold, train_loader, valid_loader, model, config, device):\n    \n    #criterion = nn.MSELoss(size_average=True)\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n    #sigmoid = nn.Sigmoid()\n    # Define your optimization algorithm. \n    # Check https://pytorch.org/docs/stable/optim.html to get more available algorithms.\n    # L2 regularization (optimizer(weight decay...) or implement by your self).\n    #optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n    #optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9, weight_decay=config['weight_decay'])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay']) \n\n    writer = SummaryWriter() # Writer of tensoboard.\n\n    if not os.path.isdir('./models'):\n        os.mkdir('./models') # Create directory of saving models.\n\n    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n\n    for epoch in range(n_epochs):\n        model.train() # Set your model to train mode.\n        loss_record = []\n        correct = 0\n        val_correct = 0\n        # tqdm is a package to visualize your training progress.\n        train_pbar = tqdm(train_loader, position=0, leave=True)\n\n        for x, y in train_pbar:\n            optimizer.zero_grad()               # Set gradient to zero.\n            x, y = x.to(device), y.to(device)   # Move your data to device. \n            pred = model(x)            \n            loss = criterion(pred, y)\n            loss.backward()                     # Compute gradient(backpropagation).\n            optimizer.step()                    # Update parameters.\n            step += 1\n            loss_record.append(loss.detach().item())\n            \n            # For MSELoss\n            #correct += (pred == y.T).float().sum()\n            \n            # For BCEWithLogitsLoss\n            #print(\"y = \", y)\n            #print(\"pred = \", pred)\n            pred = pred > 0.5\n            correct += (pred == y).float().sum()\n            #print(\"correct = \", correct)\n            \n            # Display current epoch number and loss on tqdm progress bar.\n            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n            train_pbar.set_postfix({'loss': loss.detach().item()})\n\n        mean_train_loss = sum(loss_record)/len(loss_record)\n        writer.add_scalar('Loss/train', mean_train_loss, step)\n        #print('correct = ', correct)\n        #accuracy = 100 * correct / 2000\n        accuracy = 100 * correct / 174200\n        \n        model.eval() # Set your model to evaluation mode.\n        loss_record = []\n        for x, y in valid_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                pred = model(x)                \n                loss = criterion(pred, y)\n                \n                # For MSELoss\n                #val_correct += (pred == y.T).float().sum()\n                \n                # For BCEWithLogitsLoss\n                #print(\"y = \", y)\n                #print(\"pred = \", pred)\n                pred = pred > 0.5\n                val_correct += (pred == y).float().sum()\n\n            loss_record.append(loss.item())\n            \n        mean_valid_loss = sum(loss_record)/len(loss_record)\n        #val_accuracy = 100 * val_correct / 500\n        val_accuracy = 100 * val_correct / 43550\n        \n        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n        print(f'Epoch [{epoch+1}/{n_epochs}]: Train Acc: {accuracy:.4f}, Valid Acc: {val_accuracy:.4f}')\n        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n\n        if mean_valid_loss < best_loss:\n            best_loss = mean_valid_loss\n            torch.save(model.state_dict(), config['save_path'] + fold + '_model.ckpt') # Save your best model\n            print('Saving model with loss {:.3f}...'.format(best_loss))\n            early_stop_count = 0\n        else: \n            early_stop_count += 1\n\n        if early_stop_count >= config['early_stop']:\n            print('\\nModel is not improving, so we halt the training session.')\n            print('\\nBest Loss = ' + str(best_loss))\n            return","metadata":{"id":"3xLMytuZ9Lvt","execution":{"iopub.status.busy":"2022-11-08T14:02:09.979139Z","iopub.execute_input":"2022-11-08T14:02:09.979550Z","iopub.status.idle":"2022-11-08T14:02:09.999619Z","shell.execute_reply.started":"2022-11-08T14:02:09.979515Z","shell.execute_reply":"2022-11-08T14:02:09.998522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = My_Model(input_dim=X_train.shape[1]).to(device) # put your model and data on the same computation device.\ntrainer('none', train_loader, valid_loader, model, config, device)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:02:13.688408Z","iopub.execute_input":"2022-11-08T14:02:13.688935Z","iopub.status.idle":"2022-11-08T14:03:17.099248Z","shell.execute_reply.started":"2022-11-08T14:02:13.688897Z","shell.execute_reply":"2022-11-08T14:03:17.097810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_pred(preds, file):\n    ''' Save predictions to specified file '''\n    with open(file, 'w') as fp:\n        writer = csv.writer(fp)\n        writer.writerow(['id', 'raise'])\n        for i, p in enumerate(preds):\n            writer.writerow([stock_num_mod[i], p])\n\nmodel = My_Model(input_dim=X_test.shape[1]).to(device)\nmodel.load_state_dict(torch.load(config['save_path'] + 'none_model.ckpt'))\npreds = predict(test_loader, model, device)\npreds = preds > 0.5\nsave_pred(preds, 'none_pred.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:03:17.102066Z","iopub.execute_input":"2022-11-08T14:03:17.102505Z","iopub.status.idle":"2022-11-08T14:03:17.125623Z","shell.execute_reply.started":"2022-11-08T14:03:17.102466Z","shell.execute_reply":"2022-11-08T14:03:17.124144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KFold testing\nmodel = My_Model(input_dim=X_train.shape[1]).to(device) # put your model and data on the same computation device.\nkfold = KFold(n_splits=config['k_folds'], shuffle=True)\n\nfor fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset)):\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n    train_subsampler = SubsetRandomSampler(train_ids)\n    valid_subsampler = SubsetRandomSampler(valid_ids)\n    train_loader = DataLoader(dataset, batch_size=config['batch_size'], sampler=train_subsampler, \n                              num_workers=0, pin_memory=True)\n    valid_loader = DataLoader(dataset, batch_size=config['batch_size'], sampler=valid_subsampler, \n                              num_workers=0, pin_memory=True)\n\n    trainer('Fold_' + str(fold),train_loader, valid_loader, model, config, device)\n    \n    model = My_Model(input_dim=X_test.shape[1]).to(device)\n    model.load_state_dict(torch.load(config['save_path'] + 'Fold_' + str(fold) + '_model.ckpt'))\n    preds = predict(test_loader, model, device)\n    preds = preds > 0.5\n    save_pred(preds, 'Fold_' + str(fold) + '_pred.csv')","metadata":{"id":"2YsWTYZE_EZe","execution":{"iopub.status.busy":"2022-11-08T14:03:17.127340Z","iopub.execute_input":"2022-11-08T14:03:17.127794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _shuffle(X, Y):\n    # This function shuffles two equal-length list/array, X and Y, together.\n    randomize = np.arange(len(X))\n    np.random.shuffle(randomize)\n    return (X[randomize], Y[randomize])\n\ndef _sigmoid(z):\n    # Sigmoid function can be used to calculate probability.\n    # To avoid overflow, minimum/maximum output value is set.\n    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n\ndef _f(X, w, b):\n    # This is the logistic regression function, parameterized by w and b\n    #\n    # Arguements:\n    #     X: input data, shape = [batch_size, data_dimension]\n    #     w: weight vector, shape = [data_dimension, ]\n    #     b: bias, scalar\n    # Output:\n    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n    return _sigmoid(np.matmul(np.multiply(X, X), w[:, 0]) + np.matmul(X, w[:, 1]) + b)\n\ndef _predict(X, w, b):\n    # This function returns a truth value prediction for each row of X \n    # by rounding the result of logistic regression function.\n    return np.round(_f(X, w, b)).astype(np.int)\n    \ndef _accuracy(Y_pred, Y_label):\n    # This function calculates prediction accuracy    \n    acc = 1 - np.mean(np.abs(Y_pred - Y_label.reshape(1, -1)))\n    print(acc)\n    return acc","metadata":{"id":"sXjRzgN5i4sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _cross_entropy_loss(y_pred, Y_label):\n    # This function computes the cross entropy.\n    #\n    # Arguements:\n    #     y_pred: probabilistic predictions, float vector\n    #     Y_label: ground truth labels, bool vector\n    # Output:\n    #     cross entropy, scalar\n    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))\n    return cross_entropy\n\ndef _gradient(X, Y_label, w, b):\n    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n    y_pred = _f(X, w, b)\n    pred_error = Y_label - y_pred\n    w_grad1 = -np.sum(pred_error * np.multiply(X, X).T, 1)\n    w_grad2 = -np.sum(pred_error * X.T, 1)\n    b_grad = -np.sum(pred_error)\n    return w_grad1, w_grad2, b_grad","metadata":{"id":"E6Xobcd3i4sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Regression\nw = np.zeros((data_dim, 2)) \nb = np.zeros((1,))\n\n# Some parameters for training    \nmax_iter = 100\nbatch_size = 64\nlearning_rate = 0.1\n\n# Keep the loss and accuracy at every iteration for plotting\ntrain_loss = []\ndev_loss = []\ntrain_acc = []\ndev_acc = []\n\n# Calcuate the number of parameter updates\nstep = 1\n\n# Iterative training\nfor epoch in range(max_iter):\n    # Random shuffle at the begging of each epoch\n    X_train, Y_train = _shuffle(X_train, Y_train)\n        \n    # Mini-batch training\n    for idx in range(int(np.floor(train_size / batch_size))):\n        Xb = X_train[idx*batch_size:(idx+1)*batch_size]\n        Yb = Y_train[idx*batch_size:(idx+1)*batch_size].reshape(1, -1)\n        \n        \n        # Compute the gradient\n        w_grad1, w_grad2, b_grad = _gradient(Xb, Yb, w, b)\n            \n        # gradient descent update\n        # learning rate decay with time\n        w[:, 0] = w[:, 0] - learning_rate/np.sqrt(step) * w_grad1\n        w[:, 1] = w[:, 1] - learning_rate/np.sqrt(step) * w_grad2\n        b = b - learning_rate/np.sqrt(step) * b_grad\n        \n        step = step + 1\n            \n    # Compute loss and accuracy of training set and development set\n    y_train_pred = _f(X_train, w, b)\n    Y_train_pred = np.round(y_train_pred) \n    \n    train_acc.append(_accuracy(Y_train_pred, Y_train))\n    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train.reshape(1, -1)) / train_size)\n\n    y_dev_pred = _f(X_dev, w, b)\n    Y_dev_pred = np.round(y_dev_pred)\n    \n    dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev.reshape(1, -1)) / dev_size)\n\n\nprint('Training loss: {}'.format(train_loss[-1]))\nprint('Development loss: {}'.format(dev_loss[-1]))\nprint('Training accuracy: {}'.format(train_acc[-1]))\nprint('Development accuracy: {}'.format(dev_acc[-1]))","metadata":{"id":"S66_a6AFi4sQ","outputId":"56262fd4-7c59-4f50-cfa7-8ec2f88644f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(w)\n#print(b)\nans_y = _predict(X_test, w, b)\nprint(ans_y)","metadata":{"id":"vHaYaFv9i4sQ","outputId":"d24d6e7d-dcd7-4fd7-dd66-355c7521ddec"},"execution_count":null,"outputs":[]}]}